import copy
import math
import torch
import torch.nn as nn
from timm.layers import DropPath, trunc_normal_
from pos_encoding import get_2d_sincos_pos_embed

def union_masks(masks_list):
    return torch.stack(masks_list).any(dim=0)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor
    
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def repeat_interleave_batch(x, bs, repeat):
    N = len(x) // bs
    x = torch.cat([
        torch.cat([x[i*bs:(i+1)*bs] for _ in range(repeat)], dim=0)
        for i in range(N)
    ], dim=0)
    return x


## Transformers
class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, attention_mask):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        if attention_mask is not None:
            attn = attn.masked_fill(attention_mask == 0, float('-inf'))
            
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, C) 
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

    def forward(self, x, attention_mask=None):
        x = x + self.drop_path(self.attn(self.norm1(x), attention_mask))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x
    
class Encoder_Block(nn.Module):
    def __init__(self, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4., qkv_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm):
        super().__init__() 

        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, 
                drop_path = drop_path_rate[i] if isinstance(drop_path_rate, list) else drop_path_rate,
                norm_layer=norm_layer)
            for i in range(depth)])

    def forward(self, x, pos, attention_mask=None):
        for _, block in enumerate(self.blocks):
            x = block(x + pos, attention_mask)
        return x
    
class Predictor_Block(nn.Module):
    def __init__(self, predictor_embed_dim=192, depth=4, num_heads=6, mlp_ratio=4., qkv_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):
        super().__init__()    
        self.blocks = nn.ModuleList([
            Block(
                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, 
                drop_path = drop_path_rate[i] if isinstance(drop_path_rate, list) else drop_path_rate
                )
            for i in range(depth)])
    
    def forward(self, x, pos, attention_mask=None):
        for _, block in enumerate(self.blocks):
            x = block(x + pos, attention_mask)
        return x
    

# used for target/context encoding
class MaskTransformer(nn.Module):
    def __init__(
                self, 
                embed_dim=384,
                depth=12,
                num_heads=6,
                mlp_ratio=4.0,
                qkv_bias=False,
                qk_scale=None,
                drop_rate=0.0,
                attn_drop_rate=0.0,
                drop_path_rate=0.0,
                norm_layer=nn.LayerNorm,
                init_std=0.02,
                mask_scale=(0.3, .5),
                mask_type='block',
                pos_type='sincos',
                c=8,
                p=50,
                t=50,
                leads=[0,1,2,3,4,5,6,7]
                ):
        super().__init__()

        self.mask_scale = mask_scale
        self.init_std = init_std
        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.encoder_blocks = Encoder_Block(embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=dpr, norm_layer=norm_layer)
        self.norm = nn.LayerNorm(embed_dim)
        self.c=c
        self.p = p
        self.t=t
        self.embed_dim = embed_dim
        self.W_P = nn.Linear(t,embed_dim)
        self.leads = leads
        
        if pos_type == 'learnable':
            pos_embed = torch.empty((c*p, embed_dim))
            nn.init.uniform_(pos_embed, -0.02, 0.02)
            self.pos_embed = nn.Parameter(pos_embed, requires_grad=True)
        elif pos_type == 'sincos':
            self.pos_embed = nn.Parameter(torch.zeros(c*p, embed_dim), requires_grad=False)
            pos_embed = get_2d_sincos_pos_embed(embed_dim,c,p)
            self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float())

        self.mask_type = mask_type
        # initialize the learnable token
        # trunc_normal_(self.mask_token, std=init_std)
        self.apply(self._init_weights)
        self.fix_init_weight()

    def fix_init_weight(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.encoder_blocks.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv1d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def _make_rand_mask(self, mask_scale):
        mask = torch.zeros(self.p, dtype=torch.bool)
        mask_ratio = mask_scale[0] + (mask_scale[1] - mask_scale[0]) * torch.rand(1).item()
        mask_num = int(self.p * mask_ratio)
        random_indices = torch.randperm(self.p)[:mask_num]
        mask[random_indices] = True
        return mask
    
    def _make_block_mask(self, mask_scale):
        mask = torch.zeros(self.p, dtype=torch.bool)
        for i in range(4):
            mask_ratio = mask_scale[0] + (mask_scale[1] - mask_scale[0]) * torch.rand(1).item()
            mask_num = int(self.p * mask_ratio)
            block_index = torch.randperm(self.p - mask_num)[0]
            mask[block_index:block_index+mask_num] = True
        return mask
    
    def _cross_attention_mask(self):
        size = self.c * self.p

        # Create row-wise mask
        row_mask = torch.zeros((size, size))
        for i in range(self.c):
            row_mask[i*self.p:(i+1)*self.p, i*self.p:(i+1)*self.p] = 1

        # Create column-wise mask
        col_mask = torch.zeros((size, size))
        for i in range(self.p):
            col_mask[i::self.p, i::self.p] = 1

        # Combine row-wise and column-wise masks
        combined_mask = row_mask + col_mask
        # Ensure values are binary
        combined_mask = combined_mask.clamp(max=1)

        return combined_mask

    def forward(self, x, mask=None):
        '''
        x : (bs, c, p, t)
        '''
        bs, c, p, t = x.shape
        assert c == self.c, 'Input tensor has wrong shape'
        assert p == self.p, 'Input tensor has wrong shape'
        assert t == self.t, 'Input tensor has wrong shape'

        x = x.reshape(bs, c*p, t)

        pos_embed = self.pos_embed.unsqueeze(0).expand(x.size(0), -1, -1)

        # Generate or use provided mask   
        if mask is None:
            if self.mask_type == 'random':
                mask_idx = self._make_rand_mask(self.mask_scale)
            elif self.mask_type == 'block':
                mask_idx = self._make_block_mask(self.mask_scale)
        else:
            mask_idx = mask

        vis_idx = ~mask_idx
        vis_idx = vis_idx.repeat(c)   

        # Apply projection to input tensor
        x = self.W_P(x)  # (bs, c*p, t) -> (bs, c*p, embed_dim)

        # Apply encoder block
        attention_mask = self._cross_attention_mask().to(x.device) # (c*p, c*p)

        # Slice x and positional embeddings if mask is provided
        if mask is not None:
            x = x[:,vis_idx] # (bs, c, p, embed_dim) -> (bs, c, p', embed_dim)
            pos_embed = pos_embed[:,vis_idx]  # (bs, c, p, embed_dim) -> (bs, c, p', embed_dim)
            attention_mask = attention_mask[vis_idx][:,vis_idx]
        
        x = self.encoder_blocks(x, pos_embed, attention_mask)

        # Apply normalization if specified
        if self.norm is not None:
            x = self.norm(x)

        return x, mask_idx
    
    def restrict_leads(self, vec, type='vector'):

        assert vec.size(0) == self.p * self.c, f'Input.size(0) should be c*p, vec.size(0)={vec.size(0)}'
        assert vec.dim() == 2, f'Input should be of dimension 2, vec.dim()={vec.dim()}'
        assert type in ['vector', 'matrix'] , f'type should be either vector or matrix, type={type}'

        if type == 'vector':
            vec = vec.reshape(self.c, self.p, -1)
            vec = vec[self.leads].reshape(-1, vec.size(-1))
        
        if type == 'matrix':
            assert vec.size(1) == self.c * self.p, f'Input.size(1) should be c*p, vec.size(1)={vec.size(1)}'
            vec = vec.reshape(self.c, self.p, self.c, self.p)
            vec = vec[self.leads][:,:,self.leads]
            vec = vec.reshape(len(self.leads)*self.p, len(self.leads)*self.p)

        return vec
    
    def representation(self, x):
        assert x.dim() == 3, f'Input should be of dimension 3, x.dim()={x.dim()}'
        assert x.shape[1] == len(self.leads), f'lead error'
        assert x.shape[2] == 2500, f'Input should be of shape (bs, c, 2500), x.shape[2]={x.shape[2]}'

        pos_embed = self.pos_embed
        attention_mask = self._cross_attention_mask().to(x.device) # (c*p, c*p)

        # restric leads
        if len(self.leads) < self.c:
            pos_embed = self.restrict_leads(pos_embed, type='vector')
            attention_mask = self.restrict_leads(attention_mask, type='matrix')

        bs,l,_ = x.shape
        x = x.reshape(bs,-1,50) # (bs,l,2500) -> (bs,l*p,50)
        x = self.W_P(x) # (bs,l*p,50) -> (bs,l*p,embed_dim)

        x = self.encoder_blocks(x, pos_embed, attention_mask)

        if self.norm is not None:
            x = self.norm(x)

        x = torch.mean(x, dim=1) # (bs,l*50,embed_dim) -> (bs,embed_dim)
        return x

class MaskTransformerPredictor(nn.Module):
    def __init__(
                self, 
                embed_dim=384,
                predictor_embed_dim=192,
                depth=4,
                num_heads=6,
                mlp_ratio=4.0,
                qkv_bias=False,
                qk_scale=None,
                drop_rate=0.0,
                attn_drop_rate=0.0,
                drop_path_rate=0.0,
                norm_layer=nn.LayerNorm,
                init_std=0.02,  
                pos_type='sincos',
                c=9,
                p=50,
                t=50,  
                ):
        
        super().__init__()
        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)
        self.embed_dim = embed_dim
        self.c = c
        self.p = p
        self.t = t
        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))

        if pos_type == 'learnable':
            pos_embed = torch.empty((p, predictor_embed_dim))
            nn.init.uniform_(pos_embed, -0.02, 0.02)
            self.pos_embed = nn.Parameter(pos_embed, requires_grad=False)
        elif pos_type == 'sincos':
            self.pos_embed = nn.Parameter(torch.zeros(p, predictor_embed_dim), requires_grad=False)
            pos_embed = get_2d_sincos_pos_embed(predictor_embed_dim,1,p)
            self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float())

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.predictor_blocks = Predictor_Block(predictor_embed_dim=predictor_embed_dim, depth=depth, 
                                                num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, 
                                                qk_scale=qk_scale, drop_rate=drop_rate, 
                                                attn_drop_rate=attn_drop_rate, drop_path_rate=dpr)
        
        self.predictor_norm = norm_layer(predictor_embed_dim)
        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)

        self.init_std = init_std
        trunc_normal_(self.mask_token, std=init_std)
        self.apply(self._init_weights)
        self.fix_init_weight()


    def fix_init_weight(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.predictor_blocks.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv1d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)


    def forward(self, x, mask):
        num_mask = mask.sum()
        x = self.predictor_embed(x)
        bs,_,pred_dim = x.shape
        x = x.reshape(bs*self.c, -1, pred_dim) # (bs, c*p, embed_dim) -> (bs*c, p, embed_dim)
        mask_token = self.mask_token.expand(x.size(0), num_mask, pred_dim)
        x = torch.cat([x, mask_token], dim=1)

        # reorder pos
        pos = self.pos_embed

        mask = mask
        vis_idx = (~mask).nonzero(as_tuple=True)[0]
        mask_idx = mask.nonzero(as_tuple=True)[0]
        idx = torch.cat((vis_idx, mask_idx))        
        
        pos = pos[idx]
        pos = pos.unsqueeze(0).expand(x.size(0), -1, -1)

        x = self.predictor_blocks(x, pos)
        x = self.predictor_norm(x) 
        x = self.predictor_proj(x)

        x = x.reshape(bs, -1, self.embed_dim)
        return x


class ecg_jepa(nn.Module):
    def __init__(
                self, 
                encoder_embed_dim=384,
                encoder_depth=12,
                encoder_num_heads=6,
                predictor_embed_dim=192,
                predictor_depth=4,
                predictor_num_heads=6,
                mlp_ratio=4.0,
                qkv_bias=False,
                qk_scale=None,
                drop_rate=0.0,
                attn_drop_rate=0.0,
                drop_path_rate=0.0,
                norm_layer=nn.LayerNorm,
                init_std=0.02,
                pos_type='sincos',
                mask_type='block',
                c=8,
                p=50,
                t=50,
                mask_scale=(0.15, .2),  
                leads=[0,1,2,3,4,5,6,7]
                ):
        
        super().__init__()
        self.c=c
        self.p=p
        self.t=t
        self.encoder = MaskTransformer(embed_dim=encoder_embed_dim, 
                                       depth=encoder_depth, 
                                       num_heads=encoder_num_heads, 
                                       mlp_ratio=mlp_ratio, 
                                       qkv_bias=qkv_bias, 
                                       qk_scale=qk_scale, 
                                       drop_rate=drop_rate, 
                                       attn_drop_rate=attn_drop_rate, 
                                       drop_path_rate=drop_path_rate, 
                                       norm_layer=norm_layer, 
                                       init_std=init_std, 
                                       mask_scale=mask_scale,
                                       pos_type=pos_type,
                                       mask_type=mask_type,
                                       c=self.c,
                                       p=self.p,
                                       t=self.t,
                                       leads=leads
                                       )
        
        self.target_encoder = copy.deepcopy(self.encoder)
        self.set_target_encoder()

        self.predictor = MaskTransformerPredictor(embed_dim=encoder_embed_dim, predictor_embed_dim=predictor_embed_dim, depth=predictor_depth, num_heads=predictor_num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate, norm_layer=norm_layer, init_std=init_std, c=self.c,p=self.p,t=self.t, pos_type=pos_type)
        self.loss_func = torch.nn.SmoothL1Loss()

    def set_target_encoder(self):
        for p in self.target_encoder.parameters():
            p.requires_grad = False
     
    def forward(self, x):

        bs, c, T = x.shape 
        assert T == self.p * self.t, 'Input tensor has wrong shape'
        x = x.reshape(bs, c, self.p, self.t)

        # target process
        with torch.no_grad():
            h, mask = self.target_encoder(x) # x: (bs,c, p,t), h: (bs,c*p,embed_dim)
            h = torch.nn.functional.layer_norm(h, (h.size(-1),))  # normalize over feature-dimension   
            h = h.reshape(bs, self.c, self.p, -1) # (bs,c,p,embed_dim)
            masked_h = h[:,:,mask,:]
            masked_h = masked_h.reshape(bs, -1, h.size(-1))

        # context process
        x, _ = self.encoder(x, mask) # (bs,c,p,t) -> (bs,c*p,embed_dim)
        z = self.predictor(x, mask) # (bs,c*p,embed_dim)->(bs,c*p,proj_dim)
           
        # slicing
        num_mask = mask.sum()
        z_pred = z.reshape(bs, self.c, self.p, -1)[:,:,-num_mask:,:]
        z_pred = z_pred.reshape(bs, -1, z.size(-1))

        loss = self.loss_func(z_pred, masked_h)
        return loss



